# CPU 与 GPU 体系架构笔记

## 1. CPU 体系架构

### 1.1 什么是 CPU？
CPU (Central Processing Unit)，中央处理器，是一种执行指令和处理数据的设备。

**关键特点：**
* **执行指令：** 完成基本的逻辑和算术指令，如加减乘除、数据比较等。
* **复杂功能：** 集成内存接口和外设接口，负责协调整个计算机系统的运作。
* **晶体管数量庞大：** 包含数亿甚至数十亿晶体管，是高度复杂的集成电路。

**性能优化目标：**
对于编译后的程序，性能优化目标是降低每条指令的周期数 (CPI) 和提高时钟频率，即 $\frac{cycles}{instruction} \times \frac{seconds}{cycle}$。CPI (Cycles Per Instruction，每指令周期数) 和时钟周期是相互关联的。

**桌面程序特点：**
* 轻度线程化
* 大量分支和交互
* 大量内存访问
* 实际指令数量相对较少

### 1.2 指令吞吐量：流水线 (Pipelining)

**核心思想：** 将指令分解为多个子步骤，并允许同时执行多条指令的不同子步骤，以提高指令吞吐量。
**类比：** 就像洗衣服（洗衣、烘干、折叠）一样，不同阶段可以并行进行。

**优点：**
* 显著提高指令吞吐量。

**缺点：**
* **延迟：** 增加了单条指令的执行时间（从开始到结束）。
* **流水线停顿/冒泡 (Pipeline Stalls/Bubbles)：** 当出现依赖关系或分支时，流水线可能会暂停，导致效率下降。

### 1.3 流水线冒险 (Pipeline Hazards)

流水线冒险是指在流水线处理指令时，由于指令之间存在依赖关系，导致后续指令无法在预期的时钟周期内执行的情况。

**类型：**
1.  **数据冒险 (Data Hazards)：**
    * **读后写 (RAW - Read After Write)：** 指令 $i$ 需要读取指令 $j$ 写入的寄存器。
    * **写后读 (WAR - Write After Read)：** 指令 $i$ 写入指令 $j$ 读取的寄存器。
    * **写后写 (WAW - Write After Write)：** 指令 $i$ 写入指令 $j$ 写入的寄存器。

    **解决方法：**
    * **转发/旁路 (Forwarding/Bypassing)：** 将 ALU 的输出直接连接到 ALU 的输入端，避免从寄存器文件读取，从而避免停顿。
    * **硬件互锁 (Hardware Interlock)：** 硬件检测 RAW 危险并暂停流水线，直到数据可用。

2.  **控制冒险 (Control Hazards)：**
    * 发生在分支 (branch)、跳转 (jump)、调用 (call)、返回 (return) 等改变程序执行流的指令处。CPU 难以确定下一条要执行的指令。

    **解决方法：分支预测 (Branch Prediction)**
    * **静态分支预测：** 编译器提示，或总是预测不跳转。
    * **动态分支预测：** 硬件在运行时记住分支历史，通过分支历史表 (BHT) 或分支目标缓冲 (BTB) 实现。
    * **投机执行 (Speculative Execution)：** 预测性地执行指令，如果预测错误则回滚 (rollback) 状态。

    **分支预测的惩罚：**
    * 现代 CPU 流水线深度：10-20 级。
    * 预测错误成本：10-20 个周期，需要清空流水线并重新填充。
    * 现代 CPU 分支预测准确率：通常在 90% 以上。

### 1.4 超标量 (Superscalar)

**核心思想：** 在每个时钟周期内执行多条指令。
**实现方式：** 拥有多个执行单元 (如多个 ALU、浮点单元等)，调度器 (scheduler) 在每个周期从指令流中选择多个独立的指令并行执行。
**限制：**
* **依赖关系：** 指令之间的依赖关系会限制并行度。
* **资源可用性：** 可用的执行单元数量。

### 1.5 乱序 (OoO - Out-of-Order) 执行

**核心思想：** 当一条指令无法立即执行时（例如，其所需数据尚未从内存中加载），后续的独立指令可以提前执行。
**优点：** 进一步提高指令级并行性，掩盖内存延迟。
**关键：** 在提交阶段 (Commit Stage) 仍然按程序原始顺序提交结果，以保持程序的正确性，避免副作用的乱序发生。

### 1.6 内存层次结构 (Memory Hierarchy)

**核心思想：** 利用数据局部性原理，通过多级存储器来平衡速度和容量，实现更快的平均内存访问速度。

**层次结构（从快到慢，从贵到便宜，从容量小到容量大）：**
* **寄存器 (Registers)：** 位于 CPU 内部，数量少（几十个），速度极快，CPU 直接访问。
* **缓存 (Cache)：**
    * **L1 缓存：** 几 KB - 几十 KB，几周期延迟，每个核心私有，分为指令缓存和数据缓存。
    * **L2 缓存：** 几百 KB - 几 MB，几十周期延迟，通常每个核心私有或部分共享。
    * **L3 缓存：** 几 MB - 几十 MB，几百周期延迟，通常由所有核心共享。
* **主内存 (Main Memory/RAM)：** 几 GB，几百周期延迟，CPU 主要工作区。
* **磁盘 (Disk/SSD)：** 几 TB，几百万周期延迟，用于长期存储。

**缓存局部性：**
* **时间局部性 (Temporal Locality)：** 如果一个数据项在某个时间点被引用，那么在不久的将来它很可能再次被引用。
* **空间局部性 (Spatial Locality)：** 如果一个数据项在某个时间点被引用，那么在不久的将来它附近的项很可能被引用。

**缓存行 (Cache Line)：** 缓存不是以字节为单位从内存中获取数据，而是以数据块的形式获取，这些数据块称为缓存行。典型的缓存行大小为 32 或 64 字节。

### 1.7 向量运算 (Vector Operations)

**核心思想：** 单指令多数据 (SIMD - Single Instruction, Multiple Data)，允许一条指令同时操作多个数据。
**优点：** 批量数据并行处理，提高数据密集型任务的性能。
**常见指令集：**
* **SSE (Streaming SIMD Extensions)：** 通常 128 位寄存器。
* **AVX (Advanced Vector Extensions)：** 通常 256 位或 512 位寄存器。
* 现代 CPU 通常支持 128 位、256 位或 512 位的向量寄存器。

### 1.8 多核 (Multi-Core)

**核心思想：** 在同一芯片上集成多个独立的处理器核心。
**特点：**
* 每个核心通常有自己的 L1/L2 缓存。
* 共享 L3 缓存和主内存。
* 通过线程级并行 (Thread-Level Parallelism) 提高整体性能。

### 1.9 CPU 总结

* **流水线：** 实现指令级并行。
* **分支预测和投机执行：** 处理控制冒险，减少流水线停顿。
* **超标量和乱序执行：** 进一步实现指令级并行，提高资源利用率。
* **内存层次结构：** 利用数据局部性，提高内存访问效率。
* **向量指令：** 支持数据并行，加速批量数据处理。
* **多核：** 支持线程级并行，提高多任务处理能力。

---

## 2. GPU 体系架构

### 2.1 什么是 GPU？

GPU (Graphics Processing Unit)，图形处理器，是一种专门用于执行并行计算的设备。
**特点：**
* 拥有数千个核心。
* 更适合处理高度并行化的工作负载（如图形渲染、科学计算、深度学习）。

### 2.2 CPU 与 GPU 对比

| 特性           | CPU (中央处理器)                                | GPU (图形处理器)                                |
| -------------- | ----------------------------------------------- | ----------------------------------------------- |
| **设计目标** | 通用计算、串行任务、延迟优化                    | 并行计算、吞吐量优化                            |
| **核心数量** | 少量（几核到几十核）                            | 大量（几百到几千个小核心）                      |
| **核心复杂性** | 复杂，包含大量控制逻辑、缓存、分支预测等      | 相对简单，专注于算术逻辑单元 (ALU)              |
| **缓存** | 大容量、多级缓存，用于减少延迟                  | 相对较小，主要用于数据复用和协同                |
| **执行模式** | 单线程性能强，适合复杂指令流、条件分支          | SIMD (单指令多数据)，适合数据并行、重复性任务   |
| **内存带宽** | 相对较低                                        | 极高，用于快速传输大量数据                      |
| **典型应用** | 操作系统、数据库、文字处理、需要复杂控制流的应用 | 图形渲染、科学模拟、加密货币挖矿、机器学习      |

**核心思想：** CPU 侧重于**延迟**（尽快完成单个任务），而 GPU 侧重于**吞吐量**（在单位时间内完成尽可能多的任务）。

### 2.3 GPU：从固定功能流水线到可编程流水线

早期的 GPU 具有固定的图形渲染流水线。随着技术发展，GPU 演变为高度可编程的流水线，允许程序员通过着色器 (Shader) 控制渲染的各个阶段。

* **固定功能流水线：** 硬件专用于特定图形操作，灵活性差。
* **可编程流水线：** 通过编程实现顶点着色、几何着色、像素着色等，极大地提高了图形效果的灵活性和复杂性。

### 2.4 通用 GPU (GPGPU)

GPGPU (General-Purpose computing on Graphics Processing Units) 指的是将 GPU 用于非图形的通用计算任务。
**主要编程模型/API：**
* **CUDA (Compute Unified Device Architecture)：** NVIDIA 专有，提供了 C/C++ 语言的扩展，用于在 NVIDIA GPU 上进行并行计算。
* **OpenCL (Open Computing Language)：** 开放标准，支持异构计算（CPU、GPU、FPGA 等）。

### 2.5 CUDA 编程模型

CUDA 提供了一个抽象层，将 GPU 的并行硬件抽象为更易于编程的模型。

**核心概念：**
* **主机 (Host)：** 指 CPU 及其内存。
* **设备 (Device)：** 指 GPU 及其内存。
* **内核 (Kernel)：** 在 GPU 上执行的函数。
* **网格 (Grid)：** 内核函数在 GPU 上执行时，由一个或多个线程块组成。
* **线程块 (Thread Block)：** 一组可以协作（通过共享内存和同步）的线程。线程块内的线程共享资源，可以进行同步。
* **线程 (Thread)：** GPU 上执行的最小单位。

**内存层次结构（CUDA 独有）：**
* **寄存器 (Registers)：** 每个线程私有，速度最快。
* **共享内存 (Shared Memory)：** 每个线程块内的线程共享，速度快，用于块内线程通信和数据重用。
* **本地内存 (Local Memory)：** 每个线程私有，位于全局内存中，速度慢于寄存器。
* **全局内存 (Global Memory)：** 所有线程共享，容量最大，速度最慢，是 GPU 与主机之间数据交换的主要区域。
* **常量内存 (Constant Memory)：** 所有线程共享，只读，用于存储不变的常量数据。
* **纹理内存 (Texture Memory)：** 所有线程共享，只读，针对 2D 空间局部性优化，常用于图像处理。

### 2.6 NVIDIA GPU 架构 (Fermi/Kepler/Maxwell/Pascal/Volta/Turing/Ampere 等)

NVIDIA GPU 内部由多个 **SM (Streaming Multiprocessor，流式多处理器)** 组成。

**SM (流式多处理器) 的特点：**
* 每个 SM 包含多个 **CUDA 核心**（或称为 SP - Streaming Processor，流处理器，即 ALU）。
* SM 内有调度器 (Scheduler) 负责调度线程块。
* 每个 SM 拥有自己的共享内存和寄存器文件。
* 线程以 **Warp (线程束)** 为单位执行（通常 32 个线程），同一个 Warp 中的线程执行相同的指令。

**以 NVIDIA GeForce RTX 3090 为例的规格：**
* **SM 数量：** 82 个。
* **FP32 核心数：** 每个 SM 有 128 个 FP32 核心，总计 $82 \times 128 = 10496$ 个 FP32 CUDA 核心。
* **显存：** 24 GB GDDR6X 内存。
* **内存接口：** 384 位。
* **显存带宽：** 936 GB/s (例如：1 GPU/板卡 * 384 位 * 9751 MHz 内存时钟 * 2 DDR / 8 位/字节 = 936.096 GB/s)。
* **单精度 (FP32) FLOPS：** 35.58 TFLOPS (例如：1695 MHz 核心时钟 * 1 GPU/板卡 * (82 多处理器 * 128 fp32 核心/多处理器) * 2 ops/周期 = 35.58 TFLOPS)。
* **整数运算 (INT32) TOPS：** 71.15 TOPS。
* **张量核心 (Tensor Cores) FLOPS：** 284.9 TFLOPS (稀疏)。
* **光线追踪核心 (RT Cores)。**

**指令：字节 比率：**
对于 RTX 3090，35.58 TFLOPS / 936.096 GB/s 得到 32.13 指令：1 字节。这表明 GPU 能够执行大量的计算操作，同时对内存带宽有较高的需求，但计算强度远大于内存带宽。

**注意：**
* RTX 3090 的 L1 数据/纹理缓存为 64 KB，共享内存为 48 KB，另有 16 KB 保留用于各种图形流水线操作。

### 2.7 内存和数据访问

GPU 的内存访问模式与 CPU 不同，由于其高度并行性，对内存带宽的需求极高。优化数据访问模式（如合并访问、减少全局内存访问）是 GPU 编程的关键。

---

## 3. CPU 和 GPU 趋势

* **异构计算的普及：** CPU 和 GPU 协同工作，CPU 负责串行和控制密集型任务，GPU 负责并行和数据密集型任务。
* **专用加速器：** 除了 CPU 和 GPU，还出现了越来越多的专用加速器 (如 TPU、NPU 等)，以应对特定工作负载（如 AI 推理）的需求。
* **更高集成度：** 将 CPU、GPU 和其他加速器集成到同一 SoC (System on Chip) 上，以减少数据传输延迟。
* **内存技术发展：** HBM (High Bandwidth Memory) 等高带宽内存技术被广泛应用于 GPU，以满足其巨大的内存带宽需求。
* **编程模型演进：** 出现更高级别的并行编程模型和框架，以简化异构系统的开发。

## 4. 补充知识点

* **指令集架构 (ISA - Instruction Set Architecture)：** CPU 和 GPU 所能理解和执行的指令的集合。常见的 CPU ISA 有 x86、ARM，GPU 通常有其专有的 ISA 或类似 PTX (Parallel Thread Execution) 这样的虚拟 ISA。
* **ALU (Arithmetic Logic Unit)：** 算术逻辑单元，执行算术和逻辑运算的电路。CPU 的 ALU 复杂且数量少，GPU 的 ALU 相对简单但数量多。
* **控制单元 (Control Unit)：** 负责解析指令并发出控制信号以协调 CPU 各部分的运作。CPU 的控制单元非常复杂。
* **寄存器文件 (Register File)：** CPU 或 GPU 中存储寄存器的物理单元。
* **线程束 (Warp / Wavefront)：** GPU 中一组并行执行相同指令的线程。NVIDIA GPU 通常使用 Warp (32 线程)，AMD GPU 使用 Wavefront (64 线程)。线程束的存在使得 GPU 在处理条件分支时可能面临性能损失（所有线程执行所有分支路径，但只有符合条件的线程的结果会被保留）。
* **TFLOPS (Tera Floating-point Operations Per Second)：** 万亿次浮点运算每秒，衡量浮点计算能力。
* **TOPS (Tera Operations Per Second)：** 万亿次操作每秒，通常用于衡量整数或混合精度计算能力，特别是在 AI 领域。

这份笔记涵盖了 CPU 和 GPU 体系结构的核心概念，并对RTX 3090 的数据进行了分析。理解这些概念有助于更好地优化并行计算应用程序的性能。