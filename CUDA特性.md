根据您提供的PDF内容和Markdown翻译，以下是整理后的笔记，并适当补充了相关知识点、数学公式和代码样例：

## CUDA 特性笔记

### 1. GPU 计算能力与特性支持 

CUDA 特性支持与 GPU 的计算能力（Compute Capability, CC）密切相关。不同的 GPU 架构支持的特性和性能指标有所差异。

**主要特性支持概览：**

| 功能支持                                     | 计算能力 3.5, 3.7, 5.0, 5.2, 5.3 | 计算能力 6.x, 7.x | 计算能力 8.x |
| :------------------------------------------- | :------------------------------- | :---------------- | :----------- |
| 32位整数原子操作 (全局/共享内存)           | 支持                    | 支持     | 支持  |
| 64位整数原子操作 (全局/共享内存)           | 支持                    | 支持     | 支持  |
| 32位浮点原子加法 (全局/共享内存)           | 支持                    | 支持     | 支持  |
| 64位浮点原子加法 (全局/共享内存)           | 不支持                  | 支持     | 支持  |
| Warp Vote 函数                               | 支持                    | 支持     | 支持  |
| 内存屏障函数                                 | 支持                    | 支持     | 支持  |
| 同步函数                                     | 支持                    | 支持     | 支持  |
| Surface 函数                                 | 支持                    | 支持     | 支持  |
| 统一内存编程 (Unified Memory Programming)    | 支持                    | 支持     | 支持  |
| 动态并行 (Dynamic Parallelism)               | 支持                    | 支持     | 支持  |
| 半精度浮点运算 (Half-precision)              | 不支持                  | 支持     | 支持  |
| Bfloat16 精度浮点运算                        | 不支持                  | 不支持   | 支持  |
| Tensor Cores                                 | 不支持                  | 不支持   | 支持  |
| 混合精度 Warp-Matrix 函数                    | 不支持                  | 不支持   | 支持  |
| 硬件加速 `memcpy_async`                      | 不支持                  | 不支持   | 支持  |
| 硬件加速 Split Arrive/Wait Barrier (异步屏障)| 不支持                  | 不支持   | 支持  |
| L2 缓存驻留管理                              | 不支持                  | 不支持   | 支持  |

**补充知识点：**

* **原子操作 (Atomic Operations):** 确保多个线程同时访问和修改同一内存位置时数据的一致性。例如，`atomicAdd` 函数可以原子地将一个值加到内存中的另一个值上。
    * **代码样例 (C++ CUDA `atomicAdd`):**
        ```cpp
        __device__ void atomicAddExample(int* address, int val) {
            atomicAdd(address, val);
        }

        // 在内核中调用
        // __global__ void myKernel(int* global_sum, int value_to_add) {
        //     atomicAddExample(global_sum, value_to_add);
        // }
        ```
* **Warp Vote 函数:** 允许 warp 中的线程协同操作，例如检查所有线程是否满足某个条件。
* **内存屏障函数 (Memory Fence Functions):** 确保内存操作的顺序，防止编译器和硬件乱序执行。
* **Tensor Cores:** NVIDIA Volta 架构及后续 GPU 中引入的专用硬件单元，用于加速矩阵乘法和累加运算，对深度学习训练至关重要。
    * **数学公式 (矩阵乘法):**
        对于两个矩阵 $A$ 和 $B$，它们的乘积 $C = AB$ 的元素 $C_{ij}$ 定义为：
        $$C_{ij} = \sum_{k} A_{ik} B_{kj}$$
        Tensor Cores 通过融合乘法和加法操作 (FMA) 来加速此过程，通常以低精度（如 FP16 或 BF16）进行。

### 2. GPU 技术规格 

下表总结了不同计算能力的 GPU 的主要技术规格：

| 技术规格                | CC 3.5, 3.7 | CC 5.0, 5.2, 5.3, 6.0, 6.1, 6.2 | CC 7.0, 7.2 | CC 7.5 | CC 8.0 | CC 8.6 |
| :---------------------- | :---------- | :------------------------------ | :---------- | :----- | :----- | :----- |
| 每个设备的驻留网格最大数量 (Concurrent Kernel Execution) | 32     | 16, 128, 32, 16, 128, 16   | 128  | 16  | 128  | 128  |
| 线程块网格的最大维度    | 3      | 3                      | 3      | 3     | 3     | 3     |
| 线程块网格的最大 x 维度 | $2^{31}-1$  | $2^{31}-1$             | $2^{31}-1$  | $2^{31}-1$  | $2^{31}-1$  | $2^{31}-1$  |
| 线程块的最大维度        | 3      | 3                      | 3      | 3     | 3     | 3     |
| 每个块的最大线程数      | 1024   | 1024                   | 1024   | 1024  | 1024  | 1024  |
| Warp Size               | 32     | 32                     | 32     | 32    | 32    | 32    |
| 每个 SM 的最大驻留块数  | 16, 32  | 32, 16                  | 32, 16  | 16  | 32  | 16  |
| 每个 SM 的最大驻留 warp 数 | 64 | 32               | 64  | 48  | 64  | 48  |
| 每个 SM 的最大驻留线程数 | 2048 | 1024             | 2048  | 1536  | 2048  | 1536  |
| 每个 SM 的 32 位寄存器数量 | 64 K  | 128 K, 64 K  | 64 K  | 64 K  | 64 K  | 64 K  |
| 每个线程块的最大 32 位寄存器数量 | 32 K, 64 K  | 64 K, 32 K, 64 K  | 64 K  | 64 K  | 64 K  | 64 K  |
| 每个线程的最大 32 位寄存器数量 | 255  | 255              | 255  | 255  | 255  | 255  |
| 每个 SM 的最大共享内存量 | 48 KB | 64 KB, 112 KB, 96 KB, 64 KB, 96 KB  | 96 KB  | 64 KB  | 164 KB  | 100 KB  |
| 每个线程块的最大共享内存量 | 48 KB | 96 KB, 64 KB  | 64 KB  | 163 KB  | 99 KB  | 163 KB  |
| 共享内存 Banks 数量     | 32    | 32                    | 32    | 32   | 32   | 32   |
| 每个线程的最大本地内存量 | 512 KB  | 512 KB                | 512 KB  | 512 KB  | 512 KB  | 512 KB  |
| 常量内存大小            | 64 KB   | 64 KB                 | 64 KB   | 64 KB  | 64 KB  | 64 KB  |
| 每个 SM 的常量内存缓存工作集 | 4 KB, 8 KB  | 8 KB            | 8 KB  | 8 KB  | 8 KB  | 8 KB  |
| 每个 SM 的纹理内存缓存工作集 | 24-48 KB, 12-48 KB  | 32 KB, 128 KB, 32 KB, 64 KB  | 32 KB, 64 KB  | 28 KB, 192 KB  | 28 KB, 128 KB  | 28 KB, 128 KB  |

**补充知识点：**

* **SM (Streaming Multiprocessor):** GPU 中的核心处理单元，包含多个 CUDA 核心、共享内存、寄存器文件等。
* **Warp:** CUDA 中最基本的调度单元，由 32 个线程组成。同一个 warp 中的线程在同一周期执行相同的指令。
* **寄存器 (Registers):** GPU 上最快的内存，用于存储线程的局部变量。
* **共享内存 (Shared Memory):** 每个 SM 内部的低延迟内存，可由同一线程块内的所有线程共享，用于线程间数据交换。
* **常量内存 (Constant Memory):** 一种只读内存，适用于所有线程都访问相同常量数据的情况。
* **纹理内存 (Texture Memory):** 针对二维空间局部性优化的只读内存，常用于图像处理。

### 3. CUDA 通用特性

#### 3.1 统一虚拟寻址 (Unified Virtual Addressing, UVA) 

* **支持版本:** CUDA 4.0 起支持 。
* **支持范围:** 仅由计算能力 2.0 (CC2.0) 或更高版本支持 。
* **特点:**
    * 在 64 位 CUDA 程序中默认启用 。
    * 在 UVA 之前，CPU 和 GPU 有独立的地址空间，需要显式的数据传输。
    * UVA 引入了统一的地址空间，使得 CPU 和 GPU 可以使用同一个指针来引用内存，简化了编程模型。
* **概念图示:**
    * **Separate Address Spaces:** CPU 和 GPU 有各自的全局、共享和局部内存指针，相互独立。
    * **Unified Address Space:** CPU 和 GPU 可以通过一个统一的指针 `P` 引用内存，消除了显式区分设备内存的需要。

#### 3.2 远程设备内存访问 (Remote Device Memory Access, RDMA) / GPUDirect RDMA 

* **支持版本:** CUDA 5.0 起支持 。
* **支持范围:** 仅由计算能力 3.0 (CC3.0) 或更高版本支持 。
* **特点:**
    * 允许连接到同一 PCIe 总线上的第三方设备（如 InfiniBand HCA）直接与 GPU 内存通信，无需经过 CPU 系统内存 。
    * **优势:** 显著降低了数据传输延迟，提高了带宽，对于高性能计算和数据密集型应用至关重要。
* **概念图示:**
    * **No GPUDirect RDMA:** 数据传输路径为：InfiniBand -> CPU -> 芯片组 -> GPU -> GPU 内存。
    * **GPUDirect RDMA:** 数据传输路径为：InfiniBand -> 芯片组 -> GPU -> GPU 内存，绕过了 CPU 和系统内存，实现了直接通信。

#### 3.3 动态并行 (Dynamic Parallelism) 

* **支持版本:** CUDA 5.0 起支持 。
* **支持范围:** 仅由计算能力 3.5 (CC3.5) 或更高版本支持 。
* **特点:**
    * 扩展了 CUDA 编程模型，允许在设备端（即内核内部）启动新的内核 。
    * 在 CUDA 5.0 之前，所有内核启动都必须在主机端完成 。
* **优势:**
    * 减少了主机和设备之间执行控制和数据传输的需求 。
    * 使数据驱动的并行工作更易于在 CUDA 中实现 。
    * GPU 可以根据数据动态调整，并动态启动新的线程 。
* **补充知识点:**
    * 动态并行对于不规则的并行算法非常有用，例如树遍历或自适应网格细化，因为这些算法的并行度可能在运行时动态变化。

#### 3.4 Hyper-Q 

* **支持版本:** CUDA 5.0 起支持 。
* **支持范围:** 仅由计算能力 3.5 (CC3.5) 或更高版本支持 。
* **特点:**
    * 允许来自多个 CUDA 流、多个 MPI 进程，甚至进程内多个线程同时连接到 GPU 。
    * 解决了 Fermi 架构中 GPU 只有一个硬件工作队列导致流之间无法充分并发的问题 。
    * Kepler 架构引入了网格管理单元 (Grid Management Unit)，允许管理主动调度的网格，进行调度，并保持挂起和暂停的网格 。
* **优势:**
    * 显著提高了 GPU 的利用率和吞吐量，允许多个并发任务更好地共享 GPU 资源 。
    * 在 Fermi 模型中，即使有多个流，也只能顺序执行。而 Kepler Hyper-Q 模型允许所有流并行运行，使用独立的硬件工作队列 。
    * CPU 核心可以在 Kepler 上同时运行多个任务（例如，Fermi 每次 1 个任务，Kepler 同时 32 个任务） 。
    * 支持多个 MPI 进程同时向 GPU 提交任务，并且 GPU 可以有效地调度这些任务 。

#### 3.5 多进程服务 (Multi-Process Service, MPS) 

* **支持版本:** CUDA 5.5 起支持 。
* **支持范围:**
    * 仅由计算能力 3.5 (CC3.5) 或更高版本支持 。
    * 仅在 Linux 操作系统上支持 。
    * CUDA 的统一虚拟寻址 (UVA) 功能必须可用 。
* **特点:**
    * 允许多个 MPI 进程共享一个 GPU（基于 GPU Hyper-Q） 。
    * **场景:** 适用于单个 MPI 进程无法充分利用 GPU 的情况，通过共享 GPU 资源来提高 GPU 利用率 。
    * **客户端-服务器架构:** MPS 采用客户端-服务器架构，MPS 服务器负责管理和调度来自多个客户端进程的 CUDA 工作负载 。
    * **Pre-Volta MPS:** 提供有限的隔离，优化跨进程的峰值吞吐量 。
    * **Volta MPS 增强:**
        * 降低启动延迟 。
        * 提高启动吞吐量 。
        * 通过调度器分区改进服务质量，提供更可靠的性能 。
        * 支持比 Pascal 多 3 倍的客户端 。
        * 实现了硬件隔离 。
    * **多 GPU 支持:** CUDA 7.0 起，MPS 支持多 GPU 节点，并能均匀地将任务分布到多个 GPU 上，充分利用 GPU 资源 。

#### 3.6 统一内存 (Unified Memory) 

* **支持版本:** CUDA 6.0 起支持 。
* **支持范围:** 仅由计算能力 3.0 (CC3.0) 或更高版本支持 。
* **特点:**
    * 扩展了 CUDA 编程模型，通过调用 `cudaMallocManaged()` 请求统一内存 。
    * CPU 和 GPU 都可以直接访问统一内存，无需显式调用内存复制函数 。
    * **与统一虚拟地址 (UVA) 的区别:** 统一内存需要 UVA 支持，但它提供了更高级别的抽象，管理数据在 CPU 和 GPU 内存之间的自动迁移 。
    * 统一内存为 CUDA 编程增加了灵活性 。
    * 在某些情况下，为了性能考虑，仍然需要显式内存传输 。
    * 支持通信和计算重叠 。
* **概念图示:**
    * **Developer View Today (传统模式):** CPU 系统内存和 GPU 内存是独立的，需要通过显式内存复制在两者之间传输数据。
    * **Developer View With Unified Memory:** CPU 和 GPU 共享一个统一的内存池，开发者可以使用一个指针访问数据，系统会自动处理数据迁移。
    * **CPU-GPU via PCIe vs. CPU-GPU via NVLINK:**
        * **PCIe:** 内存迁移由页面迁移引擎和访问计数器管理。
        * **NVLINK:** 在 PCIe 的基础上增加了新的 NVLINK 特性，如一致性 (Coherence)、原子操作 (Atomics) 和 ATS (Address Translation Services)，进一步优化了内存访问性能和一致性。

#### 3.7 默认流的改变 (Changes of default stream) 

* **支持版本:** CUDA 7.0 起，默认流的行为可以通过编译选项 `--default-stream per-thread` 改变 。
* **改变一 (单线程案例):**
    * **传统行为 (无 `--default-stream per-thread`):** 默认流 (NULL stream) 会阻塞其他流的操作 。这意味着默认流中的内核会等待所有前面的操作完成，并且会阻塞所有后续流中的操作，直到它自己完成。
    * **新行为 (有 `--default-stream per-thread`):** 默认流的内核可以与其他流的内核并行执行 。这提高了并发性。
    * **代码样例及结果:**
        ```cpp
        // 假设 stream 已经创建
        cudaSetDevice(0);
        doNothing<<<1, 512, 0, stream>>>(); // 非默认流
        doNothing<<<1, 512, 0, 0 >>>();      // 默认流 (NULL stream)
        //doNothing<<<1, 512 >>>();          // 等同于 doNothing<<<1, 512, 0, 0 >>>();
        doNothing<<<1, 512, 0, stream>>>(); // 非默认流
        cudaDeviceSynchronize();
        ```
        | 测试案例 | `--default-stream` | `stream` | 时间 |
        | :------- | :----------------- | :------- | :--- |
        | 案例 1   | No       | 0      | 3  |
        | 案例 2   | Yes      | 0      | 2  |
* **改变二 (多线程案例):**
    * **特点:** 在多线程情况下，通过 `--default-stream per-thread` 编译选项，每个 CPU 线程都有一个独立的默认流，并且这些默认流可以像普通流一样并行执行 。
    * **代码样例及结果:**
        ```cpp
        #pragma omp parallel
        {
            unsigned int cpu_thread_id = omp_get_thread_num();
            if (cpu_thread_id == 0){
                doNothing<<<1, 512>>>(); // 线程 0 的默认流
            } else {
                doNothing<<<1, 512>>>(); // 线程 1 的默认流
            }
            if (cpu_thread_id == 0){
                doNothing<<<1, 512>>>(); // 线程 0 的默认流
            }
        }
        ```
        | 测试案例 | 线程 0 | 线程 1 | `--default-stream` | 时间 |
        | :------- | :----- | :----- | :----------------- | :--- |
        | 案例 1   | default  | default  | No       | 3  |
        | 案例 2   | default  | default  | Yes      | 2  |

#### 3.8 其他特性 

* **兼容性支持:**
    * **PTX 代码向前兼容:** 在新的 GPU 架构上，PTX (Parallel Thread Execution) 代码会即时 (JIT) 编译成机器码 。这意味着使用旧版 CUDA Toolkit 编译的 PTX 代码可以在新版 GPU 上运行，而无需重新编译源代码。
    * **CUDA 驱动向后兼容 CUDA Toolkit:** 新的 CUDA 驱动通常支持旧版 CUDA Toolkit 编译的应用程序 。
    * CUDA 6.0 起支持 Nvidia Linux for Tegra (L4T) 。
* **已弃用和不支持的功能:**
    * **已弃用 (Deprecated):** 目前仍支持，但不建议使用，未来可能不再官方支持 。
    * **不支持 (Unsupported):** 不再官方支持的功能 。
    * CUDA 7.0 不再支持 SM_1X 架构 。
* **CUDA 7.0 起的其他支持:**
    * 支持 C++11 特性，例如 Lambda 表达式 (`[](){...}`)、`auto` 关键字等 。
    * 支持 GeForce 显卡的 MPS (Multi-Process Service) 。
    * CUDA 7.5 起支持半精度浮点数 (fp16) 。

**补充知识点：**

* **PTX (Parallel Thread Execution):** NVIDIA CUDA 的一种虚拟指令集架构。CUDA 编译器首先将 CUDA C/C++ 代码编译成 PTX，然后再由驱动程序在运行时将其 JIT 编译成特定 GPU 架构的机器码。

这份笔记应该能帮助您更好地理解 CUDA 的各项特性及其演进。